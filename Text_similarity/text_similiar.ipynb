{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning about faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Introduction:\\nRetrieval-Augmented Generation (RAG) is a technique that enhances language model generation by incorporating external knowledge.\\nThis is typically done by retrieving relevant information from a large corpus of documents and using that information to inform the generation process.\\nChallenge:\\nClients often have vast proprietary documents.\\nExtracting specific information is like finding a needle in a haystack.\\n2. GPT4-Turbo Introduction:\\nOpenAI’s GPT4-Turbo can process large documents.\\n3. Efficiency Issue:\\n“Lost In The Middle” phenomenon hampers efficiency.\\nModel forgets content in the middle of its contextual window.\\n4. Alternative Approach — Retrieval-Augmented-Generation (RAG):\\nCreate an index for each document paragraph.\\nSwiftly identify pertinent paragraphs.\\nFeed selected paragraphs into a Large Language Model (LLM) like GPT4.\\n5. Advantages:\\nPrevents information overload.\\nEnhances result quality by providing only relevant paragraphs.\\nThe Retrieval Augmented Generation (RAG) Pipeline\\nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases.\\nIt leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.\\nThe retriever here could be any of the following depending on the need for semantic retrieval or not:\\nVector database: Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.\\nGraph database: Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.\\nRegular SQL database: Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.\\nThe image below from Damien Benveniste, PhD talks a bit about the difference between using Graph vs Vector database for RAG.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "documents = loader.load()\n",
    "Text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = Text_splitter.split_documents(documents)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/69q1v4892n7cc021jv93kh5m0000gn/T/ipykernel_55524/988078411.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"deepseek-r1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1197fc890>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"deepseek-r1\")\n",
    "db = FAISS.from_documents(docs,embeddings)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2e0f5119-861e-4f46-8e2b-713be96050bc', metadata={'source': 'speech.txt'}, page_content='Introduction:\\nRetrieval-Augmented Generation (RAG) is a technique that enhances language model generation by incorporating external knowledge.\\nThis is typically done by retrieving relevant information from a large corpus of documents and using that information to inform the generation process.\\nChallenge:\\nClients often have vast proprietary documents.\\nExtracting specific information is like finding a needle in a haystack.\\n2. GPT4-Turbo Introduction:\\nOpenAI’s GPT4-Turbo can process large documents.\\n3. Efficiency Issue:\\n“Lost In The Middle” phenomenon hampers efficiency.\\nModel forgets content in the middle of its contextual window.\\n4. Alternative Approach — Retrieval-Augmented-Generation (RAG):\\nCreate an index for each document paragraph.\\nSwiftly identify pertinent paragraphs.\\nFeed selected paragraphs into a Large Language Model (LLM) like GPT4.\\n5. Advantages:\\nPrevents information overload.\\nEnhances result quality by providing only relevant paragraphs.\\nThe Retrieval Augmented Generation (RAG) Pipeline\\nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases.\\nIt leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.\\nThe retriever here could be any of the following depending on the need for semantic retrieval or not:\\nVector database: Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.\\nGraph database: Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.\\nRegular SQL database: Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.\\nThe image below from Damien Benveniste, PhD talks a bit about the difference between using Graph vs Vector database for RAG.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what are theEfficiency Issues?\"\n",
    "docs= db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
